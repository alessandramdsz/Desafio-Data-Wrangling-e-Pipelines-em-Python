# üöÄ Pipeline ETL de An√°lise de Vendas - Desafio Desafio-Data-Wrangling-e-Pipelines-em-Python - SQUAD ENIAC
# Membros: Alessandra, Aline, Caroline, Clara, Dayane, Elaine, Maria Elacide, Tandara

[![Status](https://img.shields.io/badge/Status-Completo-green.svg)]()
[![Tecnologias](https://img.shields.io/badge/Linguagem-Python-blue.svg)]()

## Descri√ß√£o desse projeto

Este projeto implementa um **Pipeline de ETL (Extra√ß√£o, Transforma√ß√£o e Carga)** em Python para processar e estruturar dados brutos de vendas (Arquivo `vendas.csv`). Almejamos limpar, padronizar e enriquecer os dados para que possam ser carregados em um banco de dados (SQLite), prontos para an√°lise e relat√≥rios da empresa em quest√£o.

O script executa as seguintes etapas principais:
1.  **Extra√ß√£o:** Leitura do arquivo CSV hospedado no Google Drive, pasta do projeto (via `gdown`).
2.  **Transforma√ß√£o:** Limpeza e padroniza√ß√£o de datas, corre√ß√£o de tipos de dados, tratamento de valores nulos/negativos e c√°lculo de `valor_total`.
3.  **Carga:** Cria√ß√£o e preenchimento de duas tabelas em SQLite (`tb_vendas` e `tb_clientes_unicos`), estabelecendo um relacionamento de chave estrangeira.

Em resumo, o fluxo do pipeline √©: 
**CSV (Google Drive) $\rightarrow$ Extra√ß√£o (`gdown`/`pandas`) $\rightarrow$ Transforma√ß√£o $\rightarrow$ Carga (SQLite)**.

## Tecnologias que foram usadas

* **Linguagem de programa√ß√£o principal:** Python
* **Bibliotecas:**
    * `pandas`: Para manipula√ß√£o e *data wrangling* dos dados.
    * `sqlite3`: Para a fase de carga (terceira etapa) para cria√ß√£o e manipula√ß√£o do banco de dados.
    * `numpy`: Em conjunto com o pandas.
    * `gdown`: Para fazer o download do CSV diretamente do Google Drive no Colab.
* **Ambiente:** Google Colab

## Estrutura desse Pipeline (Etapa de transforma√ß√µes)

As seguintes transforma√ß√µes de *Data Wrangling* foram aplicadas nos dados:

| Etapa | Descri√ß√£o da Transforma√ß√£o |
| :--- | :--- |
| **Extra√ß√£o** | Leitura e filtros iniciais: `preco_unitario > 100`, ordena√ß√£o decrescente pelo pre√ßo. |
| **Data** | Padroniza√ß√£o da coluna `data_venda` para o formato `YYYY-MM-DD`, al√©m de padronizar o type. |
| **Tratamento dos nulo** | Substitui√ß√£o de valores nulos na coluna `categoria` por "N√£o informado". |
| **Tipo de dados** | Corre√ß√£o da coluna `quantidade`, substituindo o texto 'tr√™s' por '3' e convertendo-a para o tipo inteiro. |
| **Valores negativos** | Remo√ß√£o de linhas onde `preco_unitario` era negativo, para limpeza. |
| **C√°lculo** | Cria√ß√£o da coluna `valor_total` (calculada por `quantidade * preco_unitario`). |

## Instru√ß√µes de como executar o projeto

Este c√≥digo foi projetado para rodar no **Google Colab**, utilizando-se de upload e acesso ao Google Drive.

**Requisitos:**

* Acesso ao Google Colab (ou qualquer ambiente Python com as bibliotecas sinalizadas instaladas).
* O ou um arquivo CSV de vendas com a estrutura esperada pelo script.

**Passos para executar no Colab:**

Este c√≥digo foi otimizado para rodar no Google Colab, fazendo uso das bibliotecas gdown, pandas e sqlite3.

1. **Abra o Notebook:** Cole o conte√∫do do arquivo `eniac_desafio_data_wrangling_e_pipelines_em_python.py` em uma c√©lula de c√≥digo do Google Colab.
2. **Suba o arquivo CSV:** O script tenta ler um arquivo chamado `vendas.csv`. Voc√™ deve garantir que o CSV est√° no mesmo diret√≥rio de execu√ß√£o ou alterar o `arquivo_id` e `url` no trecho:
    ```python
    arquivo_id = "13IPaHOFEnvDkApg4E6jwyhvHvaOHFOLX"  # N√£o esque√ßa de ajustar o caminho para novos arquivos, ok?
    url = f"https://drive.google.com/uc?id={arquivo_id}"
    gdown.download(url, output, quiet=False)
    ```
       Dica: Voc√™ pode simplesmente subir o arquivo `vendas.csv` na sess√£o do Colab ou usar o `gdown` como configurado.
       O c√≥digo tamb√©m dispon√≠vel aqui https://drive.google.com/file/d/1PbVcJZ1MOek4_OWEP0lDb6ZZEckqZfAv/view?usp=sharing ou no notebook https://colab.research.google.com/drive/1n45URFAAXMoZqlSYHwV5z-hhZUP4kNUR?usp=sharing
   
4.  **Execute as c√©lulas:** Execute as c√©lulas de c√≥digo sequencialmente.
5.  **Download do banco:** Ap√≥s executar, o script faz o download autom√°tico do banco de dados gerado:
    ```python
    from google.colab import files
    files.download('vendas.db')
    ```

## Consultas SQL de valida√ß√£o

Ap√≥s a Carga (Load), o script valida o resultado com consultas SQL. A consulta principal gerada √©:

```sql
SELECT categoria,
       ROUND(SUM(valor_total), 2) AS total_vendas
FROM tb_vendas
GROUP BY categoria
ORDER BY total_vendas DESC;

---

üìÑ Licen√ßa
Este projeto √© de c√≥digo aberto e est√° sob a licen√ßa MIT.

üìß Contato
Alessandra Machado - @alessandramdsz

Link do projeto: [https://github.com/alessandramdsz/Desafio-Data-Wrangling-e-Pipelines-em-Python]
